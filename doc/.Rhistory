# Chunk 14
# create data frame to organize results
res_cv_trees_low <- as.data.frame(res_cv_trees_low)
colnames(res_cv_trees_low) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
cv_results_trees_low = data.frame(hyper_grid_trees, res_cv_trees_low)
# look at top 5 models with lowest AUC
cv_results_trees_low[order(cv_results_trees_low$mean_AUC, decreasing = TRUE), ][1:5, ]
# Chunk 15
# cross validation results for low dimensional data: mean_error
cv_results_trees_low %>% ggplot(aes(x = as.factor(cp), y = mean_error,
ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# cross validation results for low dimensional data: mean_AUC
cv_results_trees_low %>% ggplot(aes(x = as.factor(cp), y = mean_AUC,
ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Chunk 16
best_cp_low <- cv_results_trees_low$cp[cv_results_trees_low$mean_AUC ==
max(cv_results_trees_low$mean_AUC)]
best_cp_low
# Chunk 17: weights
# imbalanced dataset requires weights
# to be used in the trained model
weights_high <- rep(NA, length(df_high$A))
for (v in unique(df_high$A)){
weights_high[df_high$A == v] = 0.5 * length(df_high$A) / length(df_high$A[df_high$A == v])
}
weights_low <- rep(NA, length(df_low$A))
for (v in unique(df_low$A)){
weights_low[df_low$A == v] = 0.5 * length(df_low$A) / length(df_low$A[df_low$A == v])
}
# Chunk 18
start.time_propensity_score_high <- Sys.time()
# create tree model for high dimensional data with best cp parameter
tree_high <- rpart(A ~ . - Y, method = "class", data = df_high, cp = best_cp_high)
# calculate propensity scores
prop_score_high <- predict(tree_high, newdata = df_high[, -2], type = "prob")[, 2]
end.time_propensity_score_high <- Sys.time()
time_propensity_score_high <- end.time_propensity_score_high - start.time_propensity_score_high
time_propensity_score_high
# Chunk 19
# plot tree
rpart.plot(tree_high, type = 1, digits = 3, fallen.leaves = TRUE)
# Chunk 20
start.time_propensity_score_low <- Sys.time()
# create tree model for low dimensional data with best cp parameter
tree_low <- rpart(A ~ . - Y, method = "class", data = df_low, cp = best_cp_low)
# calculate propensity scores
prop_score_low <- predict(tree_low, newdata = df_low[, -2], type = "prob")[, 2]
end.time_propensity_score_low <- Sys.time()
time_propensity_score_low <- end.time_propensity_score_low - start.time_propensity_score_low
time_propensity_score_low
# Chunk 21
# plot tree
rpart.plot(tree_low, type = 1, digits = 3, fallen.leaves = TRUE)
K = 5
quintiles <- seq(0, 1, by = 1/K)
start.time_stratification_high <- Sys.time()
df_high <- cbind(df_high, prop_score_high)
quintile_values_high <- rep(NA, length(quintiles))
for (i in 1:length(quintiles)){
quintile_values_high[i] <- quantile(prop_score_high, quintiles[i])
}
# values of quintiles for high data
quintile_values_high
df_high$quintile_class_high <- rep(NA, nrow(df_high))
# assign quintile class to each observation
for (i in 1:nrow(df_high)){
if ((quintile_values_high[1] <= df_high$prop_score_high[i]) &
(df_high$prop_score_high[i] < quintile_values_high[2])) {
df_high$quintile_class_high[i] <- 1
} else if ((quintile_values_high[2] <= df_high$prop_score_high[i]) &
(df_high$prop_score_high[i] < quintile_values_high[3])) {
df_high$quintile_class_high[i] <- 2
} else if ((quintile_values_high[3] <= df_high$prop_score_high[i]) &
(df_high$prop_score_high[i] < quintile_values_high[4])) {
df_high$quintile_class_high[i] <- 3
} else if ((quintile_values_high[4] <= df_high$prop_score_high[i]) &
(df_high$prop_score_high[i] < quintile_values_high[5])) {
df_high$quintile_class_high[i] <- 4
} else if ((quintile_values_high[5] <= df_high$prop_score_high[i]) &
(df_high$prop_score_high[i] <= quintile_values_high[6])) {
df_high$quintile_class_high[i] <- 5
}
}
summary_high = expand.grid(
A = c(0, 1),
quintile = c(1, 2, 3, 4, 5),
n = NA,
prop = NA,
avg_y = NA
)
for (i in 1:nrow(summary_high)) {
subset <- df_high[(df_high$A == summary_high$A[i]) & (df_high$quintile_class_high == summary_high$quintile[i]), ]
summary_high$n[i] = nrow(subset)
summary_high$prop[i] = summary_high$n[i]/nrow(df_high)
summary_high$avg_y[i] = mean(subset$Y)
}
for (i in 1:nrow(summary_high)) {
if (is.nan(summary_high$avg_y[i]) == TRUE) {
summary_high$avg_y[i] <- 0
}
}
# this table records the mean response in each quintile; needed for stratification
summary_high
quntile_prop_high <- summary_high %>% group_by(quintile) %>% summarise(sum = sum(n)/nrow(df_high))
# this table records the proportions for each quintile; also needed for stratification
quntile_prop_high
ATE_stratification_high = quntile_prop_high$sum[1]*(summary_high$avg_y[2] - summary_high$avg_y[1]) +
quntile_prop_high$sum[2]*(summary_high$avg_y[4] - summary_high$avg_y[3]) +
quntile_prop_high$sum[3]*(summary_high$avg_y[6] - summary_high$avg_y[5]) +
quntile_prop_high$sum[4]*(summary_high$avg_y[8] - summary_high$avg_y[7]) +
quntile_prop_high$sum[5]*(summary_high$avg_y[10] - summary_high$avg_y[9])
ATE_stratification_high
end.time_stratification_high <- Sys.time()
time_stratification_high <- end.time_stratification_high - start.time_stratification_high
time_stratification_high
summary_high = expand.grid(
A = c(0, 1),
quintile = seq(1, K, by = 1),
n = NA,
prop = NA,
avg_y = NA
)
summary_high
summary_high = expand.grid(
A = c(0, 1),
quintile = seq(1, K, by = 1),
n = NA,
prop = NA,
avg_y = NA
)
summary_high
setwd("~/Fall2020-Project4-group-4")
setwd("~/Documents/Fall2020-Project4-group-4")
setwd("~/GitHub/Fall2020-Project4-group-4/doc")
setwd("~/GitHub/Fall2020-Project4-group-4/doc")
df_high <- read.csv("../data/highDim_dataset.csv")
df_low <- read.csv("../data/lowDim_dataset.csv")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: wd
setwd("~/GitHub/Fall2020-Project4-group-4/doc")
# Chunk 3
packages.used <- c("dplyr", "ggplot2", "WeightedROC", "rpart", "rpart.plot")
# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
# install additional packages
if(length(packages.needed) > 0){
install.packages(packages.needed, dependencies = TRUE)
}
library(dplyr)
library(ggplot2)
library(WeightedROC)
library(rpart)
library(rpart.plot)
library(base)
# Chunk 4
df_high <- read.csv("../data/highDim_dataset.csv")
df_low <- read.csv("../data/lowDim_dataset.csv")
# Chunk 5
K <- 5  # number of CV folds
sample.reweight <- TRUE # run sample reweighting in model training
# setting the following to false loads data generated from a previous run
# this data is the same in each run due to a set seed
run.cv.trees_high <- FALSE # run cross-validation on the training set for trees on high dim data
run.cv.trees_low <- FALSE # run cross-validation on the training set for trees on low dim data
# Chunk 6
# hyperparameters for trees
hyper_grid_trees <- expand.grid(
cp = c(2^(0), 2^(-1), 2^(-2), 2^(-3), 2^(-4),
2^(-5), 2^(-6), 2^(-7), 2^(-8), 2^(-9),
2^(-10), 2^(-11), 2^(-12), 2^(-13), 2^(-14),
2^(-15), 2^(-16), 2^(-17), 0, -2^(0))
)
# Chunk 7: loadlib_trees
source("../lib/train_trees.R")
source("../lib/test_trees.R")
source("../lib/cross_validation_trees.R")
# Chunk 8: features
# features are the predictors: V1 - Vp
# column 1 is the response Y
# column 2 is the treatment A
feature_train_high = df_high[, -1:-2]
label_train_high = df_high[, 2]
feature_train_low = df_low[, -1:-2]
label_train_low = df_low[, 2]
# Chunk 9: runcv_trees_high
set.seed(5243)
if(run.cv.trees_high){
res_cv_trees_high <- matrix(0, nrow = nrow(hyper_grid_trees), ncol = 4)
for(i in 1:nrow(hyper_grid_trees)){
cat("complexity = ", hyper_grid_trees$cp[i], "\n", sep = "")
res_cv_trees_high[i,] <- cv.function(features = feature_train_high, labels = label_train_high,
cp = hyper_grid_trees$cp[i],
K, reweight = sample.reweight)
save(res_cv_trees_high, file = "../output/res_cv_trees_high.RData")
}
}else{
load("../output/res_cv_trees_high.RData")
}
# Chunk 10: runcv_trees_low
set.seed(5243)
if(run.cv.trees_low){
res_cv_trees_low <- matrix(0, nrow = nrow(hyper_grid_trees), ncol = 4)
for(i in 1:nrow(hyper_grid_trees)){
cat("complexity = ", hyper_grid_trees$cp[i], "\n", sep = "")
res_cv_trees_low[i,] <- cv.function(features = feature_train_low, labels = label_train_low,
cp = hyper_grid_trees$cp[i],
K, reweight = sample.reweight)
save(res_cv_trees_low, file="../output/res_cv_trees_low.RData")
}
}else{
load("../output/res_cv_trees_low.RData")
}
# Chunk 11
# create data frame to organize results
res_cv_trees_high <- as.data.frame(res_cv_trees_high)
colnames(res_cv_trees_high) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
cv_results_trees_high = data.frame(hyper_grid_trees, res_cv_trees_high)
# look at top 5 models with highest AUC
cv_results_trees_high[order(cv_results_trees_high$mean_AUC, decreasing = TRUE), ][1:5, ]
# Chunk 12
# cross validation results for high dimensional data: mean_error
cv_results_trees_high %>% ggplot(aes(x = as.factor(cp), y = mean_error,
ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# cross validation results for high dimensional data: mean_AUC
cv_results_trees_high %>% ggplot(aes(x = as.factor(cp), y = mean_AUC,
ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Chunk 13
best_cp_high <- cv_results_trees_high$cp[cv_results_trees_high$mean_AUC ==
max(cv_results_trees_high$mean_AUC)]
best_cp_high
# Chunk 14
# create data frame to organize results
res_cv_trees_low <- as.data.frame(res_cv_trees_low)
colnames(res_cv_trees_low) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
cv_results_trees_low = data.frame(hyper_grid_trees, res_cv_trees_low)
# look at top 5 models with lowest AUC
cv_results_trees_low[order(cv_results_trees_low$mean_AUC, decreasing = TRUE), ][1:5, ]
# Chunk 15
# cross validation results for low dimensional data: mean_error
cv_results_trees_low %>% ggplot(aes(x = as.factor(cp), y = mean_error,
ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# cross validation results for low dimensional data: mean_AUC
cv_results_trees_low %>% ggplot(aes(x = as.factor(cp), y = mean_AUC,
ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Chunk 16
best_cp_low <- cv_results_trees_low$cp[cv_results_trees_low$mean_AUC ==
max(cv_results_trees_low$mean_AUC)]
best_cp_low
# Chunk 17: weights
# imbalanced dataset requires weights
# to be used in the trained model
weights_high <- rep(NA, length(df_high$A))
for (v in unique(df_high$A)){
weights_high[df_high$A == v] = 0.5 * length(df_high$A) / length(df_high$A[df_high$A == v])
}
weights_low <- rep(NA, length(df_low$A))
for (v in unique(df_low$A)){
weights_low[df_low$A == v] = 0.5 * length(df_low$A) / length(df_low$A[df_low$A == v])
}
# Chunk 18
start.time_propensity_score_high <- Sys.time()
# create tree model for high dimensional data with best cp parameter
tree_high <- rpart(A ~ . - Y, method = "class", data = df_high, cp = best_cp_high)
# calculate propensity scores
prop_score_high <- predict(tree_high, newdata = df_high[, -2], type = "prob")[, 2]
end.time_propensity_score_high <- Sys.time()
time_propensity_score_high <- end.time_propensity_score_high - start.time_propensity_score_high
time_propensity_score_high
# Chunk 19
# plot tree
rpart.plot(tree_high, type = 1, digits = 3, fallen.leaves = TRUE)
# Chunk 20
start.time_propensity_score_low <- Sys.time()
# create tree model for low dimensional data with best cp parameter
tree_low <- rpart(A ~ . - Y, method = "class", data = df_low, cp = best_cp_low)
# calculate propensity scores
prop_score_low <- predict(tree_low, newdata = df_low[, -2], type = "prob")[, 2]
end.time_propensity_score_low <- Sys.time()
time_propensity_score_low <- end.time_propensity_score_low - start.time_propensity_score_low
time_propensity_score_low
# Chunk 21
# plot tree
rpart.plot(tree_low, type = 1, digits = 3, fallen.leaves = TRUE)
# Chunk 22
K = 4
quintiles <- seq(0, 1, by = 1/K)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: wd
setwd("~/GitHub/Fall2020-Project4-group-4/doc")
# Chunk 3
packages.used <- c("dplyr", "ggplot2", "WeightedROC", "rpart", "rpart.plot")
# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
# install additional packages
if(length(packages.needed) > 0){
install.packages(packages.needed, dependencies = TRUE)
}
library(dplyr)
library(ggplot2)
library(WeightedROC)
library(rpart)
library(rpart.plot)
library(base)
# Chunk 4
df_high <- read.csv("../data/highDim_dataset.csv")
df_low <- read.csv("../data/lowDim_dataset.csv")
# Chunk 5
K <- 5  # number of CV folds
sample.reweight <- TRUE # run sample reweighting in model training
# setting the following to false loads data generated from a previous run
# this data is the same in each run due to a set seed
run.cv.trees_high <- FALSE # run cross-validation on the training set for trees on high dim data
run.cv.trees_low <- FALSE # run cross-validation on the training set for trees on low dim data
# Chunk 6
# hyperparameters for trees
hyper_grid_trees <- expand.grid(
cp = c(2^(0), 2^(-1), 2^(-2), 2^(-3), 2^(-4),
2^(-5), 2^(-6), 2^(-7), 2^(-8), 2^(-9),
2^(-10), 2^(-11), 2^(-12), 2^(-13), 2^(-14),
2^(-15), 2^(-16), 2^(-17), 0, -2^(0))
)
# Chunk 7: loadlib_trees
source("../lib/train_trees.R")
source("../lib/test_trees.R")
source("../lib/cross_validation_trees.R")
# Chunk 8: features
# features are the predictors: V1 - Vp
# column 1 is the response Y
# column 2 is the treatment A
feature_train_high = df_high[, -1:-2]
label_train_high = df_high[, 2]
feature_train_low = df_low[, -1:-2]
label_train_low = df_low[, 2]
# Chunk 9: runcv_trees_high
set.seed(5243)
if(run.cv.trees_high){
res_cv_trees_high <- matrix(0, nrow = nrow(hyper_grid_trees), ncol = 4)
for(i in 1:nrow(hyper_grid_trees)){
cat("complexity = ", hyper_grid_trees$cp[i], "\n", sep = "")
res_cv_trees_high[i,] <- cv.function(features = feature_train_high, labels = label_train_high,
cp = hyper_grid_trees$cp[i],
K, reweight = sample.reweight)
save(res_cv_trees_high, file = "../output/res_cv_trees_high.RData")
}
}else{
load("../output/res_cv_trees_high.RData")
}
# Chunk 10: runcv_trees_low
set.seed(5243)
if(run.cv.trees_low){
res_cv_trees_low <- matrix(0, nrow = nrow(hyper_grid_trees), ncol = 4)
for(i in 1:nrow(hyper_grid_trees)){
cat("complexity = ", hyper_grid_trees$cp[i], "\n", sep = "")
res_cv_trees_low[i,] <- cv.function(features = feature_train_low, labels = label_train_low,
cp = hyper_grid_trees$cp[i],
K, reweight = sample.reweight)
save(res_cv_trees_low, file="../output/res_cv_trees_low.RData")
}
}else{
load("../output/res_cv_trees_low.RData")
}
# Chunk 11
# create data frame to organize results
res_cv_trees_high <- as.data.frame(res_cv_trees_high)
colnames(res_cv_trees_high) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
cv_results_trees_high = data.frame(hyper_grid_trees, res_cv_trees_high)
# look at top 5 models with highest AUC
cv_results_trees_high[order(cv_results_trees_high$mean_AUC, decreasing = TRUE), ][1:5, ]
# Chunk 12
# cross validation results for high dimensional data: mean_error
cv_results_trees_high %>% ggplot(aes(x = as.factor(cp), y = mean_error,
ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# cross validation results for high dimensional data: mean_AUC
cv_results_trees_high %>% ggplot(aes(x = as.factor(cp), y = mean_AUC,
ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Chunk 13
best_cp_high <- cv_results_trees_high$cp[cv_results_trees_high$mean_AUC ==
max(cv_results_trees_high$mean_AUC)]
best_cp_high
# Chunk 14
# create data frame to organize results
res_cv_trees_low <- as.data.frame(res_cv_trees_low)
colnames(res_cv_trees_low) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
cv_results_trees_low = data.frame(hyper_grid_trees, res_cv_trees_low)
# look at top 5 models with lowest AUC
cv_results_trees_low[order(cv_results_trees_low$mean_AUC, decreasing = TRUE), ][1:5, ]
# Chunk 15
# cross validation results for low dimensional data: mean_error
cv_results_trees_low %>% ggplot(aes(x = as.factor(cp), y = mean_error,
ymin = mean_error - sd_error, ymax = mean_error + sd_error)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# cross validation results for low dimensional data: mean_AUC
cv_results_trees_low %>% ggplot(aes(x = as.factor(cp), y = mean_AUC,
ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) +
geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Chunk 16
best_cp_low <- cv_results_trees_low$cp[cv_results_trees_low$mean_AUC ==
max(cv_results_trees_low$mean_AUC)]
best_cp_low
# Chunk 17: weights
# imbalanced dataset requires weights
# to be used in the trained model
weights_high <- rep(NA, length(df_high$A))
for (v in unique(df_high$A)){
weights_high[df_high$A == v] = 0.5 * length(df_high$A) / length(df_high$A[df_high$A == v])
}
weights_low <- rep(NA, length(df_low$A))
for (v in unique(df_low$A)){
weights_low[df_low$A == v] = 0.5 * length(df_low$A) / length(df_low$A[df_low$A == v])
}
# Chunk 18
start.time_propensity_score_high <- Sys.time()
# create tree model for high dimensional data with best cp parameter
tree_high <- rpart(A ~ . - Y, method = "class", data = df_high, cp = best_cp_high)
# calculate propensity scores
prop_score_high <- predict(tree_high, newdata = df_high[, -2], type = "prob")[, 2]
end.time_propensity_score_high <- Sys.time()
time_propensity_score_high <- end.time_propensity_score_high - start.time_propensity_score_high
time_propensity_score_high
# Chunk 19
# plot tree
rpart.plot(tree_high, type = 1, digits = 3, fallen.leaves = TRUE)
# Chunk 20
start.time_propensity_score_low <- Sys.time()
# create tree model for low dimensional data with best cp parameter
tree_low <- rpart(A ~ . - Y, method = "class", data = df_low, cp = best_cp_low)
# calculate propensity scores
prop_score_low <- predict(tree_low, newdata = df_low[, -2], type = "prob")[, 2]
end.time_propensity_score_low <- Sys.time()
time_propensity_score_low <- end.time_propensity_score_low - start.time_propensity_score_low
time_propensity_score_low
# Chunk 21
# plot tree
rpart.plot(tree_low, type = 1, digits = 3, fallen.leaves = TRUE)
# Chunk 22
K = 4
quintiles <- seq(0, 1, by = 1/K)
# Chunk 23
start.time_stratification_high <- Sys.time()
df_high <- cbind(df_high, prop_score_high)
quintile_values_high <- rep(NA, length(quintiles))
for (i in 1:length(quintiles)){
quintile_values_high[i] <- quantile(prop_score_high, quintiles[i])
}
# values of quintiles for high data
quintile_values_high
df_high$quintile_class_high <- rep(NA, nrow(df_high))
# assign quintile class to each observation
for (i in 1:nrow(df_high)){
if ((quintile_values_high[1] <= df_high$prop_score_high[i]) &
(df_high$prop_score_high[i] < quintile_values_high[2])) {
df_high$quintile_class_high[i] <- 1
} else if ((quintile_values_high[2] <= df_high$prop_score_high[i]) &
(df_high$prop_score_high[i] < quintile_values_high[3])) {
df_high$quintile_class_high[i] <- 2
} else if ((quintile_values_high[3] <= df_high$prop_score_high[i]) &
(df_high$prop_score_high[i] < quintile_values_high[4])) {
df_high$quintile_class_high[i] <- 3
} else if ((quintile_values_high[4] <= df_high$prop_score_high[i]) &
(df_high$prop_score_high[i] < quintile_values_high[5])) {
df_high$quintile_class_high[i] <- 4
} else if ((quintile_values_high[5] <= df_high$prop_score_high[i]) &
(df_high$prop_score_high[i] <= quintile_values_high[6])) {
df_high$quintile_class_high[i] <- 5
}
}
summary_high = expand.grid(
A = c(0, 1),
quintile = seq(1, K, by = 1),
n = NA,
prop = NA,
avg_y = NA
)
for (i in 1:nrow(summary_high)) {
subset <- df_high[(df_high$A == summary_high$A[i]) & (df_high$quintile_class_high == summary_high$quintile[i]), ]
summary_high$n[i] = nrow(subset)
summary_high$prop[i] = summary_high$n[i]/nrow(df_high)
summary_high$avg_y[i] = mean(subset$Y)
}
for (i in 1:nrow(summary_high)) {
if (is.nan(summary_high$avg_y[i]) == TRUE) {
summary_high$avg_y[i] <- 0
}
}
# this table records the mean response in each quintile; needed for stratification
summary_high
quntile_prop_high <- summary_high %>% group_by(quintile) %>% summarise(sum = sum(n)/nrow(df_high))
# this table records the proportions for each quintile; also needed for stratification
quntile_prop_high
ATE_stratification_high = quntile_prop_high$sum[1]*(summary_high$avg_y[2] - summary_high$avg_y[1]) +
quntile_prop_high$sum[2]*(summary_high$avg_y[4] - summary_high$avg_y[3]) +
quntile_prop_high$sum[3]*(summary_high$avg_y[6] - summary_high$avg_y[5]) +
quntile_prop_high$sum[4]*(summary_high$avg_y[8] - summary_high$avg_y[7]) +
quntile_prop_high$sum[5]*(summary_high$avg_y[10] - summary_high$avg_y[9])
ATE_stratification_high
end.time_stratification_high <- Sys.time()
time_stratification_high <- end.time_stratification_high - start.time_stratification_high
time_stratification_high
ATE_stratification_high
summary_high
library(rpart)
?rpart
