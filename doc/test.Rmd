---
title: "Project 4: Causal Inference Algorithms Evaluation"
author: "Group 4: Zhenglei Chen, Jaival Desai, Qinzhe Hu, Levi Lee, Luyao Sun, Xinyi Wei"
date: "12/02/2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

First, we set working directories, install required libraries and import the data. 

```{r wd}
setwd("~/GitHub/Fall2020-Project4-group-4/doc")
```

```{r}
packages.used <- c("dplyr", "ggplot2", "WeightedROC", "rpart", "rpart.plot")

# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))

# install additional packages
if(length(packages.needed) > 0){
   install.packages(packages.needed, dependencies = TRUE)
}

library(dplyr)
library(ggplot2)
library(WeightedROC)
library(rpart)
library(rpart.plot)
library(base)
```

```{r}
df_high <- read.csv("../data/highDim_dataset.csv")
df_low <- read.csv("../data/lowDim_dataset.csv")
```
## Introduction 



## About the Data



## Background: Trees



## Cross-Validation



### Step 1: Set Controls and Establish Hyperparameters



```{r}
K <- 5  # number of CV folds
sample.reweight <- TRUE # run sample reweighting in model training

# setting the following to false loads data generated from a previous run
# this data is the same in each run due to a set seed

run.cv.trees_high <- FALSE # run cross-validation on the training set for trees on high dim data

run.cv.trees_low <- FALSE # run cross-validation on the training set for trees on low dim data
```



```{r}
# hyperparameters for trees
hyper_grid_trees <- expand.grid(
  cp = c(2^(0), 2^(-1), 2^(-2), 2^(-3), 2^(-4), 
         2^(-5), 2^(-6), 2^(-7), 2^(-8), 2^(-9), 
         2^(-10), 2^(-11), 2^(-12), 2^(-13), 2^(-14), 
         2^(-15), 2^(-16), 2^(-17), 0, -2^(0))
)
```

### Step 2: Cross-Validate the Hyperparameters




```{r loadlib_trees, echo=FALSE}
source("../lib/train_trees.R") 
source("../lib/test_trees.R")
source("../lib/cross_validation_trees.R")
```


```{r features}
# features are the predictors: V1 - Vp
# column 1 is the response Y
# column 2 is the treatment A

feature_train_high = df_high[, -1:-2]
label_train_high = df_high[, 2]

feature_train_low = df_low[, -1:-2]
label_train_low = df_low[, 2]
```

#### High Dimensional Data



```{r runcv_trees_high, message = FALSE, }
set.seed(5243)

if(run.cv.trees_high){
  res_cv_trees_high <- matrix(0, nrow = nrow(hyper_grid_trees), ncol = 4)
  for(i in 1:nrow(hyper_grid_trees)){
    cat("complexity = ", hyper_grid_trees$cp[i], "\n", sep = "")
    res_cv_trees_high[i,] <- cv.function(features = feature_train_high, labels = label_train_high,
                                         cp = hyper_grid_trees$cp[i], 
                                         K, reweight = sample.reweight)
  save(res_cv_trees_high, file = "../output/res_cv_trees_high.RData")
  }
}else{
  load("../output/res_cv_trees_high.RData")
}
```

#### Low Dimensional Data


```{r runcv_trees_low, message = FALSE}
set.seed(5243)

if(run.cv.trees_low){
  res_cv_trees_low <- matrix(0, nrow = nrow(hyper_grid_trees), ncol = 4)
  for(i in 1:nrow(hyper_grid_trees)){
    cat("complexity = ", hyper_grid_trees$cp[i], "\n", sep = "")
    res_cv_trees_low[i,] <- cv.function(features = feature_train_low, labels = label_train_low, 
                                        cp = hyper_grid_trees$cp[i], 
                                        K, reweight = sample.reweight)
  save(res_cv_trees_low, file="../output/res_cv_trees_low.RData")
  }
}else{
  load("../output/res_cv_trees_low.RData")
}
```

### Step 3: Visualize CV Error and AUC



#### High Dimensional Data 





```{r}
# create data frame to organize results
res_cv_trees_high <- as.data.frame(res_cv_trees_high) 
colnames(res_cv_trees_high) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
cv_results_trees_high = data.frame(hyper_grid_trees, res_cv_trees_high)

# look at top 5 models with highest AUC
cv_results_trees_high[order(cv_results_trees_high$mean_AUC, decreasing = TRUE), ][1:5, ]
```

```{r, out.width = "85%", fig.align = 'center', echo=FALSE}
# round hyperparameter values
cp_2 <- signif(hyper_grid_trees$cp, 3)


# cross validation results for high dimensional data: mean_error 
cv_results_trees_high %>% 
  ggplot(aes(x = as.factor(cp_2), y = mean_error,
            ymin = mean_error - sd_error, 
            ymax = mean_error + sd_error)) + 
  labs(title="Mean of Error with Different cp Values for High Dimensional Data ",
       x="cp", y = "Mean of Error")+
  geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

# cross validation results for high dimensional data: mean_AUC
cv_results_trees_high %>% 
  ggplot(aes(x = as.factor(cp_2), y = mean_AUC,
             ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) + 
  labs(title="Mean of AUC with Different cp Values for High Dimensional Data ",
       x="cp", y = "Mean of AUC")+
  geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
best_cp_high <- cv_results_trees_high$cp[cv_results_trees_high$mean_AUC ==
                                           max(cv_results_trees_high$mean_AUC)]

best_cp_high
```

#### Low Dimensional Data 





```{r}
# create data frame to organize results
res_cv_trees_low <- as.data.frame(res_cv_trees_low) 
colnames(res_cv_trees_low) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
cv_results_trees_low = data.frame(hyper_grid_trees, res_cv_trees_low)

# look at top 5 models with lowest AUC
cv_results_trees_low[order(cv_results_trees_low$mean_AUC, decreasing = TRUE), ][1:5, ]
```

```{r, out.width = "85%", fig.align = 'center', echo=FALSE}
# cross validation results for low dimensional data: mean_error 
cv_results_trees_low %>% 
  ggplot(aes(x = as.factor(cp_2), y = mean_error,
            ymin = mean_error - sd_error, 
            ymax = mean_error + sd_error)) + 
  labs(title="Mean of Error with Different cp Values for Low Dimensional Data ",
       x="cp", y = "Mean of Error")+
  geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

# cross validation results for low dimensional data: mean_AUC
cv_results_trees_low %>% 
  ggplot(aes(x = as.factor(cp_2), y = mean_AUC,
             ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) + 
  labs(title="Mean of AUC with Different cp Values for Low Dimensional Data ",
       x="cp", y = "Mean of AUC")+
  geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


```{r}
best_cp_low <- cv_results_trees_low$cp[cv_results_trees_low$mean_AUC ==
                                           max(cv_results_trees_low$mean_AUC)]

best_cp_low
```

## Propensity Score Estimation




```{r weights}
# imbalanced dataset requires weights 
# to be used in the trained model

weights_high <- rep(NA, length(df_high$A))
for (v in unique(df_high$A)){
  weights_high[df_high$A == v] = 0.5 * length(df_high$A) / length(df_high$A[df_high$A == v])
}


weights_low <- rep(NA, length(df_low$A))
for (v in unique(df_low$A)){
  weights_low[df_low$A == v] = 0.5 * length(df_low$A) / length(df_low$A[df_low$A == v])
}
```

#### High Dimensional Data 



```{r}
start.time_propensity_score_high <- Sys.time()

# create tree model for high dimensional data with best cp parameter
tree_high <- rpart(A ~ . - Y, method = "class", data = df_high, cp = best_cp_high)

# calculate propensity scores
prop_score_high <- predict(tree_high, newdata = df_high[, -2], type = "prob")[, 2]

end.time_propensity_score_high <- Sys.time()
time_propensity_score_high <- end.time_propensity_score_high - start.time_propensity_score_high
time_propensity_score_high
```

```{r, out.width = "85%", fig.align = 'center', echo=FALSE}
# plot tree
rpart.plot(tree_high, type = 1, digits = 3, fallen.leaves = TRUE)
```

#### Low Dimensional Data

```{r}
start.time_propensity_score_low <- Sys.time()

# create tree model for low dimensional data with best cp parameter
tree_low <- rpart(A ~ . - Y, method = "class", data = df_low, cp = best_cp_low)

# calculate propensity scores
prop_score_low <- predict(tree_low, newdata = df_low[, -2], type = "prob")[, 2]

end.time_propensity_score_low <- Sys.time()
time_propensity_score_low <- end.time_propensity_score_low - start.time_propensity_score_low
time_propensity_score_low
```

```{r, out.width = "85%", fig.align = 'center', echo=FALSE}
# plot tree
rpart.plot(tree_low, type = 1, digits = 3, fallen.leaves = TRUE)
```

## ATE Estimation




### Stratification



```{r}
K = 5
quintiles <- seq(0, 1, by = 1/K)
```

#### High Dimensional Data 



```{r}
start.time_stratification_high <- Sys.time()

df_high <- cbind(df_high, prop_score_high)
quintile_values_high <- rep(NA, length(quintiles))
  
for (i in 1:length(quintiles)){
  quintile_values_high[i] <- quantile(prop_score_high, quintiles[i])
}

# values of quintiles for high data
quintile_values_high

df_high$quintile_class_high <- rep(NA, nrow(df_high))

# assign quintile class to each observation
for (i in 1:nrow(df_high)){
  if ((quintile_values_high[1] <= df_high$prop_score_high[i]) & 
      (df_high$prop_score_high[i] < quintile_values_high[2])) {
    df_high$quintile_class_high[i] <- 1
  } else if ((quintile_values_high[2] <= df_high$prop_score_high[i]) & 
             (df_high$prop_score_high[i] < quintile_values_high[3])) {
    df_high$quintile_class_high[i] <- 2
  } else if ((quintile_values_high[3] <= df_high$prop_score_high[i]) & 
             (df_high$prop_score_high[i] < quintile_values_high[4])) {
    df_high$quintile_class_high[i] <- 3 
  } else if ((quintile_values_high[4] <= df_high$prop_score_high[i]) & 
             (df_high$prop_score_high[i] < quintile_values_high[5])) {
    df_high$quintile_class_high[i] <- 4
  } else if ((quintile_values_high[5] <= df_high$prop_score_high[i]) & 
             (df_high$prop_score_high[i] <= quintile_values_high[6])) {
    df_high$quintile_class_high[i] <- 5
  }
}

summary_high = expand.grid(
  A = c(0, 1), 
  quintile = seq(1, K, by = 1), 
  n = NA, 
  prop = NA, 
  avg_y = NA
)

for (i in 1:nrow(summary_high)) {
  subset <- df_high[(df_high$A == summary_high$A[i]) & 
                      (df_high$quintile_class_high == summary_high$quintile[i]), ]
  summary_high$n[i] = nrow(subset)
  summary_high$prop[i] = summary_high$n[i]/nrow(df_high)
  summary_high$avg_y[i] = mean(subset$Y)
}


for (i in 1:nrow(summary_high)) {
  if (is.nan(summary_high$avg_y[i]) == TRUE) {
    summary_high$avg_y[i] <- 0 
  }
}

# this table records the mean response in each quintile; needed for stratification
summary_high

quntile_prop_high <- summary_high %>% group_by(quintile) %>% summarise(sum = sum(n)/nrow(df_high))

# this table records the proportions for each quintile; also needed for stratification
quntile_prop_high

ATE_stratification_high = quntile_prop_high$sum[1]*(summary_high$avg_y[2] - summary_high$avg_y[1]) + 
  quntile_prop_high$sum[2]*(summary_high$avg_y[4] - summary_high$avg_y[3]) + 
  quntile_prop_high$sum[3]*(summary_high$avg_y[6] - summary_high$avg_y[5]) + 
  quntile_prop_high$sum[4]*(summary_high$avg_y[8] - summary_high$avg_y[7]) + 
  quntile_prop_high$sum[5]*(summary_high$avg_y[10] - summary_high$avg_y[9])


ATE_stratification_high

end.time_stratification_high <- Sys.time()
time_stratification_high <- end.time_stratification_high - start.time_stratification_high
time_stratification_high
```


#### Low Dimensional Data


```{r}
start.time_stratification_low <- Sys.time()

df_low <- cbind(df_low, prop_score_low)
quintile_values_low <- rep(NA, length(quintiles))

for (i in 1:length(quintiles)){
  quintile_values_low[i] <- quantile(prop_score_low, quintiles[i])
}

# values of quintiles for low data
quintile_values_low

df_low$quintile_class_low <- rep(NA, nrow(df_low))

# assign quintile class to each observation
for (i in 1:nrow(df_low)){
  if ((quintile_values_low[1] <= df_low$prop_score_low[i]) & 
      (df_low$prop_score_low[i] < quintile_values_low[2])) {
    df_low$quintile_class_low[i] <- 1
  } else if ((quintile_values_low[2] <= df_low$prop_score_low[i]) & 
             (df_low$prop_score_low[i] < quintile_values_low[3])) {
    df_low$quintile_class_low[i] <- 2
  } else if ((quintile_values_low[3] <= df_low$prop_score_low[i]) & 
             (df_low$prop_score_low[i] < quintile_values_low[4])) {
    df_low$quintile_class_low[i] <- 3 
  } else if ((quintile_values_low[4] <= df_low$prop_score_low[i]) & 
             (df_low$prop_score_low[i] < quintile_values_low[5])) {
    df_low$quintile_class_low[i] <- 4
  } else if ((quintile_values_low[5] <= df_low$prop_score_low[i]) & 
             (df_low$prop_score_low[i] <= quintile_values_low[6])) {
    df_low$quintile_class_low[i] <- 5
  }
}


summary_low = expand.grid(
  A = c(0, 1), 
  quintile = c(1, 2, 3, 4, 5), 
  n = NA, 
  prop = NA, 
  avg_y = NA
)

for (i in 1:nrow(summary_low)) {
  subset <- df_low[(df_low$A == summary_low$A[i]) & 
                     (df_low$quintile_class_low == summary_low$quintile[i]), ]
  summary_low$n[i] = nrow(subset)
  summary_low$prop[i] = summary_low$n[i]/nrow(df_low)
  summary_low$avg_y[i] = mean(subset$Y)
}


for (i in 1:nrow(summary_low)) {
  if (is.nan(summary_low$avg_y[i]) == TRUE) {
    summary_low$avg_y[i] <- 0 
  }
}

# this table records the mean response in each quintile; needed for stratification
summary_low

quntile_prop_low <- summary_low %>% group_by(quintile) %>% summarise(sum = sum(n)/nrow(df_low))

# this table records the proportions for each quintile; also needed for stratification
quntile_prop_low

ATE_stratification_low = quntile_prop_low$sum[1]*(summary_low$avg_y[2] - summary_low$avg_y[1]) + 
  quntile_prop_low$sum[2]*(summary_low$avg_y[4] - summary_low$avg_y[3]) + 
  quntile_prop_low$sum[3]*(summary_low$avg_y[6] - summary_low$avg_y[5]) + 
  quntile_prop_low$sum[4]*(summary_low$avg_y[8] - summary_low$avg_y[7]) + 
  quntile_prop_low$sum[5]*(summary_low$avg_y[10] - summary_low$avg_y[9])

ATE_stratification_low

end.time_stratification_low <- Sys.time()
time_stratification_low <- end.time_stratification_low - start.time_stratification_low
time_stratification_low
```


### Regression Adjustment



#### High Dimensional Data 


```{r}
start.time_regression_adjustment_high <- Sys.time()

ps_RA_high <- predict(tree_high, df_high, type = "prob")
high_data_ps <- cbind(ps_RA_high, df_high)
pred_high <- lm(Y ~ A + ps_RA_high, data = high_data_ps)
summary(pred_high)
ATE_regression_adjustment_high = pred_high$coefficients[2]
ATE_regression_adjustment_high

end.time_regression_adjustment_high <- Sys.time()
time_regression_adjustment_high <- end.time_regression_adjustment_high - 
  start.time_regression_adjustment_high
time_regression_adjustment_high
```

#### Low Dimensional Data


```{r}
start.time_regression_adjustment_low <- Sys.time()

ps_RA_low <- predict(tree_low, df_low, type = "prob")
low_data_ps <- cbind(ps_RA_low, df_low)
pred_low <- lm(Y ~ A + ps_RA_low, data = low_data_ps)
summary(pred_low)
ATE_regression_adjustment_low = pred_low$coefficients[2]
ATE_regression_adjustment_low

end.time_regression_adjustment_low <- Sys.time()
time_regression_adjustment_low <- end.time_regression_adjustment_low - 
  start.time_regression_adjustment_low
time_regression_adjustment_low
```

### Stratification and Regression Adjustment


#### High Dimensional Data 


```{r}
start.time_stratification_regression_adjustment_high <- Sys.time()

lm_beta_high <- rep(NA, K)

for (i in 1:K){
  subset <- df_high[df_high$quintile_class_high == i, ]
  
  if (nrow(subset) == 0) { 
    # if the quintile is empty, let the coefficient for A automatically be 0
    lm_beta_high[i] <- 0
  } else if (sum(subset$prop_score_high) == 0) { 
    # if the propensity scores in the quintile are all 0, 
    # let the coefficient for A automatically be 0
    lm_beta_low[i] <- 0
  } else {
    # otherwise, run a linear model on the subset
    lm <- lm(Y ~ A + prop_score_high, data = subset)
    lm_beta_high[i] <- as.numeric(lm$coefficients[2])
  }
}

lm_beta_high 


ATE_stratification_regression_adjustment_high <- quntile_prop_high$sum[1]*lm_beta_high[1] + 
  quntile_prop_high$sum[2]*lm_beta_high[2] + 
  quntile_prop_high$sum[3]*lm_beta_high[3] + 
  quntile_prop_high$sum[4]*lm_beta_high[4] + 
  quntile_prop_high$sum[5]*lm_beta_high[5]


ATE_stratification_regression_adjustment_high

end.time_stratification_regression_adjustment_high <- Sys.time()

time_stratification_regression_adjustment_high <-  
  end.time_stratification_regression_adjustment_high - 
  start.time_stratification_regression_adjustment_high

time_stratification_regression_adjustment_high
```

#### Low Dimensional Data

```{r}
start.time_stratification_regression_adjustment_low <- Sys.time()

lm_beta_low <- rep(NA, 5)

for (i in 1:K){
  subset <- df_low[df_low$quintile_class_low == i, ]
  
  if (nrow(subset) == 0) {
    # if the quintile is empty, let the coefficient for A automatically be 0
    lm_beta_low[i] <- 0
  } else if (sum(subset$prop_score_low) == 0) {
    # if the propensity scores in the quintile are all 0
    # let the coefficient for A automatically be 0
    lm_beta_low[i] <- 0
  } else {
    # otherwise, run a linear model on the subset
    lm <- lm(Y ~ A + prop_score_low, data = subset)
    lm_beta_low[i] <- as.numeric(lm$coefficients[2])
  }
}

lm_beta_low 

ATE_stratification_regression_adjustment_low <- quntile_prop_low$sum[1]*lm_beta_low[1] + 
  quntile_prop_low$sum[2]*lm_beta_low[2] + 
  quntile_prop_low$sum[3]*lm_beta_low[3] + 
  quntile_prop_low$sum[4]*lm_beta_low[4] + 
  quntile_prop_low$sum[5]*lm_beta_low[5]

ATE_stratification_regression_adjustment_low

end.time_stratification_regression_adjustment_low <- Sys.time()

time_stratification_regression_adjustment_low <- 
  end.time_stratification_regression_adjustment_low -
  start.time_stratification_regression_adjustment_low

time_stratification_regression_adjustment_low
```

## Results 


### ATE Results

```{r}
# summarize table of results - ATE
ATE_true_high <- -3
ATE_true_low <- 2.5
ATE <- matrix(c(ATE_true_high, ATE_stratification_high, 
                ATE_regression_adjustment_high, 
                ATE_stratification_regression_adjustment_high, 
                ATE_true_low, 
                ATE_stratification_low, 
                ATE_regression_adjustment_low, 
                ATE_stratification_regression_adjustment_low), 
              ncol = 2, byrow = F)

colnames(ATE) <- c("High Dimensional Data", "Low Dimensional Data")

rownames(ATE) <- c("True", "Stratification", "Regression Adjustment",
                   "Stratification + Regression Adjustment")

ATE <- as.table(ATE)

ATE
```

### Runtime results
```{r}
# summarize table of results - Run Time
time <- matrix(c(time_propensity_score_high, 
                 time_stratification_high, 
                 time_regression_adjustment_high, 
                 time_stratification_regression_adjustment_high, 
                 time_propensity_score_low, 
                 time_stratification_low, 
                 time_regression_adjustment_low, 
                 time_stratification_regression_adjustment_low), 
               ncol = 2, byrow = F)

colnames(time) <- c("High Dimensional Data", "Low Dimensional Data")

rownames(time) <- c("Propensity Score Estimation", "Stratification", "Regression Adjustment",
                    "Stratification + Regression Adjustment")

time <- as.table(time)

time
```


## Conclusion 


## References 

 * Austin, Peter C. 2011. “An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies.” Multivariate Behavioral Research 46 (3): 399–424.

 * Chan, David & Ge, Rong & Gershony, Ori & Hesterberg, Tim & Lambert, Diane. (2010). Evaluating online ad campaigns in a pipeline: Causal models at scale. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 7-16. 10.1145/1835804.1835809. 

 * D'Agostino RB Jr. Propensity score methods for bias reduction in the comparison of a treatment to a non-randomized control group. Stat Med. 1998 Oct 15;17(19):2265-81. doi: 10.1002/(sici)1097-0258(19981015)17:19<2265::aid-sim918>3.0.co;2-b. PMID: 9802183.

 * Lunceford, Jared K, and Marie Davidian. 2004. “Stratification and Weighting via the Propensity Score in Estimation of Causal Treatment Effects a Comparative Study.” Statistics in Medicine 23 (19): 2937–60.
