---
title: "Project 4: Causal Inference Algorithms Evaluation"
author: "Group 4: Zhenglei Chen, Jaival Desai, Qinzhe Hu, Levi Lee, Luyao Sun, Xinyi Wei"
date: "12/02/2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

First, we set working directories, install required libraries and import the data. 

```{r wd}
setwd("C:/Users/wuyam/Desktop/Fall2020-Project4-group-4")
```

```{r}
packages.used <- c("dplyr", "ggplot2", "WeightedROC", "rpart", "rpart.plot")

# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))

# install additional packages
if(length(packages.needed) > 0){
   install.packages(packages.needed, dependencies = TRUE)
}

library(dplyr)
library(ggplot2)
library(WeightedROC)
library(rpart)
library(rpart.plot)
library(base)
```

```{r}
df_high <- read.csv("C:/Users/wuyam/Desktop/Fall2020-Project4-group-4/data/highDim_dataset.csv")
df_low <- read.csv("C:/Users/wuyam/Desktop/Fall2020-Project4-group-4/data/lowDim_dataset.csv")
```
## Introduction 

In this project, we are looking for the best algorithm method for causal inference of propensity scores to see how close the estimated ATEs and true ATEs would be. The algorithms we use to compare ATE in this project are regression adjustment, stratification, and the combination of them. For the estimation of propensity score, we use the regression trees. The results of estimated ATE of best algorhitms, combination of stratification and regression adjustment, are -2.504545 for high dimension data set and 3.058512 for low dimension data set, which are really closed to the true ATE: -2.5 for high dimension and 3 for low dimension. We notice that regression adjustment also performs well on estimating the ATEs: 3.05324 for low dimension and -2.527116 for high dimension. However, the estimated ATEs of stratification are significantly different from the true value. 

## About the Data

There are two attached data set for this project, named *highDim_dataset* and *lowDim_dataset*. For high dimension data, there are 2000 observations, 185 variables, 1 treatment indicator and 1 response variable. For low dimension data, there are 475 observations, 22 variables, 1 treatment indicator and 1 response variable

## Background: Trees

Regression tree is the method we used for estimating propensity scores. In this project, regression tree is designed for classification for the quantile of propensity score. The tree seperated by every variable which will significantly influence the quantile that the propensity scores will falls in. The more layers tree has, the more precise quantile and propensity score will be.

## Cross-Validation

 Usually we make a 5-folds cross validation for stratification.

### Step 1: Set Controls and Establish Hyperparameters

In order to get a balanced tree model, which is neither too much layers or too much bias, we need to find the best hyperparameter for the tree.

```{r}
K <- 5  # number of CV folds
sample.reweight <- TRUE # run sample reweighting in model training

# setting the following to false loads data generated from a previous run
# this data is the same in each run due to a set seed

run.cv.trees_high <- FALSE # run cross-validation on the training set for trees on high dim data

run.cv.trees_low <- FALSE # run cross-validation on the training set for trees on low dim data
```

The index of power shows the layers we may want to check.

```{r}
# hyperparameters for trees
hyper_grid_trees <- expand.grid(
  cp = c(2^(0), 2^(-1), 2^(-2), 2^(-3), 2^(-4), 
         2^(-5), 2^(-6), 2^(-7), 2^(-8), 2^(-9), 
         2^(-10), 2^(-11), 2^(-12), 2^(-13), 2^(-14), 
         2^(-15), 2^(-16), 2^(-17), 0, -2^(0))
)
```

### Step 2: Cross-Validate the Hyperparameters

The hyperparameter affects the performance of regression tree model which will directly influence the results of propensity scores. To get the best hyperparameter, we need to check the mean AUC and mean error of model under different hyperparameter value.

```{r loadlib_trees, echo=FALSE}
source("C:/Users/wuyam/Desktop/Fall2020-Project4-group-4/lib/train_trees.R") 
source("C:/Users/wuyam/Desktop/Fall2020-Project4-group-4/lib/test_trees.R")
source("C:/Users/wuyam/Desktop/Fall2020-Project4-group-4/lib/cross_validation_trees.R")
```


```{r features}
# features are the predictors: V1 - Vp
# column 1 is the response Y
# column 2 is the treatment A

feature_train_high = df_high[, -1:-2]
label_train_high = df_high[, 2]

feature_train_low = df_low[, -1:-2]
label_train_low = df_low[, 2]
```

#### High Dimensional Data

Creating the cross validation for high dimension.

```{r runcv_trees_high, message = FALSE, }
set.seed(5243)

if(run.cv.trees_high){
  res_cv_trees_high <- matrix(0, nrow = nrow(hyper_grid_trees), ncol = 4)
  for(i in 1:nrow(hyper_grid_trees)){
    cat("complexity = ", hyper_grid_trees$cp[i], "\n", sep = "")
    res_cv_trees_high[i,] <- cv.function(features = feature_train_high, labels = label_train_high,
                                         cp = hyper_grid_trees$cp[i], 
                                         K, reweight = sample.reweight)
  save(res_cv_trees_high, file = "C:/Users/wuyam/Desktop/Fall2020-Project4-group-4/output/res_cv_trees_high.RData")
  }
}else{
  load("C:/Users/wuyam/Desktop/Fall2020-Project4-group-4/output/res_cv_trees_high.RData")
}
```

#### Low Dimensional Data

Creating the cross validation for low dimension.

```{r runcv_trees_low, message = FALSE}
set.seed(5243)

if(run.cv.trees_low){
  res_cv_trees_low <- matrix(0, nrow = nrow(hyper_grid_trees), ncol = 4)
  for(i in 1:nrow(hyper_grid_trees)){
    cat("complexity = ", hyper_grid_trees$cp[i], "\n", sep = "")
    res_cv_trees_low[i,] <- cv.function(features = feature_train_low, labels = label_train_low, 
                                        cp = hyper_grid_trees$cp[i], 
                                        K, reweight = sample.reweight)
  save(res_cv_trees_low, file="C:/Users/wuyam/Desktop/Fall2020-Project4-group-4/output/res_cv_trees_low.RData")
  }
}else{
  load("C:/Users/wuyam/Desktop/Fall2020-Project4-group-4/output/res_cv_trees_low.RData")
}
```

### Step 3: Visualize CV Error and AUC

In this section, we will use the bar plot to figure out the highest mean AUC and its complex parameter.

#### High Dimensional Data 

The best complex parameter for high dimension data is  0.0078125.

```{r}
# create data frame to organize results
res_cv_trees_high <- as.data.frame(res_cv_trees_high) 
colnames(res_cv_trees_high) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
cv_results_trees_high = data.frame(hyper_grid_trees, res_cv_trees_high)

# look at top 5 models with highest AUC
cv_results_trees_high[order(cv_results_trees_high$mean_AUC, decreasing = TRUE), ][1:5, ]
```

```{r, out.width = "85%", fig.align = 'center', echo=FALSE}
# cross validation results for high dimensional data: mean_error 
cv_results_trees_high %>% ggplot(aes(x = as.factor(cp), y = mean_error,
            ymin = mean_error - sd_error, ymax = mean_error + sd_error)) + 
  geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

# cross validation results for high dimensional data: mean_AUC
cv_results_trees_high %>% ggplot(aes(x = as.factor(cp), y = mean_AUC,
            ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) + 
  geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
best_cp_high <- cv_results_trees_high$cp[cv_results_trees_high$mean_AUC ==
                                           max(cv_results_trees_high$mean_AUC)]

best_cp_high
```

#### Low Dimensional Data 

The best complex parameter for low dimension data is  7.629395e-06.

```{r}
# create data frame to organize results
res_cv_trees_low <- as.data.frame(res_cv_trees_low) 
colnames(res_cv_trees_low) <- c("mean_error", "sd_error", "mean_AUC", "sd_AUC")
cv_results_trees_low = data.frame(hyper_grid_trees, res_cv_trees_low)

# look at top 5 models with lowest AUC
cv_results_trees_low[order(cv_results_trees_low$mean_AUC, decreasing = TRUE), ][1:5, ]
```

```{r, out.width = "85%", fig.align = 'center', echo=FALSE}
# cross validation results for low dimensional data: mean_error 
cv_results_trees_low %>% ggplot(aes(x = as.factor(cp), y = mean_error,
            ymin = mean_error - sd_error, ymax = mean_error + sd_error)) + 
  geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

# cross validation results for low dimensional data: mean_AUC 
cv_results_trees_low %>% ggplot(aes(x = as.factor(cp), y = mean_AUC,
            ymin = mean_AUC - sd_AUC, ymax = mean_AUC + sd_AUC)) + 
  geom_crossbar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


```{r}
best_cp_low <- cv_results_trees_low$cp[cv_results_trees_low$mean_AUC ==
                                           max(cv_results_trees_low$mean_AUC)]

best_cp_low
```

## Propensity Score Estimation

In this section, we will use regression tree to get our propensity score based on the variables from two datasets.

```{r weights}
# imbalanced dataset requires weights 
# to be used in the trained model

weights_high <- rep(NA, length(df_high$A))
for (v in unique(df_high$A)){
  weights_high[df_high$A == v] = 0.5 * length(df_high$A) / length(df_high$A[df_high$A == v])
}


weights_low <- rep(NA, length(df_low$A))
for (v in unique(df_low$A)){
  weights_low[df_low$A == v] = 0.5 * length(df_low$A) / length(df_low$A[df_low$A == v])
}
```

#### High Dimensional Data 

```{r}
start.time_propensity_score_high <- Sys.time()

# create tree model for high dimensional data with best cp parameter
tree_high <- rpart(A ~ . - Y, method = "class", data = df_high, cp = best_cp_high)

# calculate propensity scores
prop_score_high <- predict(tree_high, newdata = df_high[, -2], type = "prob")[, 2]

end.time_propensity_score_high <- Sys.time()
time_propensity_score_high <- end.time_propensity_score_high - start.time_propensity_score_high
time_propensity_score_high
```

```{r, out.width = "85%", fig.align = 'center', echo=FALSE}
# plot tree
rpart.plot(tree_high, type = 1, digits = 3, fallen.leaves = TRUE)
```

#### Low Dimensional Data

```{r}
start.time_propensity_score_low <- Sys.time()

# create tree model for low dimensional data with best cp parameter
tree_low <- rpart(A ~ . - Y, method = "class", data = df_low, cp = best_cp_low)

# calculate propensity scores
prop_score_low <- predict(tree_low, newdata = df_low[, -2], type = "prob")[, 2]

end.time_propensity_score_low <- Sys.time()
time_propensity_score_low <- end.time_propensity_score_low - start.time_propensity_score_low
time_propensity_score_low
```

```{r, out.width = "85%", fig.align = 'center', echo=FALSE}
# plot tree
rpart.plot(tree_low, type = 1, digits = 3, fallen.leaves = TRUE)
```

## ATE Estimation

$$\Delta=E(Y_1-Y_0)=E(Y_1)-E(Y_0)$$
This is the formula of Average Treatment Effect(ATE). We use it for control experiment. ATE can measure the average difference of results between the control group and treating group.

### Stratification

we divide the whole propensity score into 5 strata. In each strata, we calculate the mean propensity score of control group and treating group, and then we calculate the difference of mean propensity scores of two groups of each strata. Finally, we calcualte the weighted average of mean difference of 5 stratas.

```{r}
K = 5
quintiles <- seq(0, 1, by = 1/K)
```

#### High Dimensional Data 

The ATE of high dimension is -1.87638.  
Runtime of high dimension is 0.621835 secs.

```{r}
start.time_stratification_high <- Sys.time()

df_high <- cbind(df_high, prop_score_high)
quintile_values_high <- rep(NA, length(quintiles))
  
for (i in 1:length(quintiles)){
  quintile_values_high[i] <- quantile(prop_score_high, quintiles[i])
}

# values of quintiles for high data
quintile_values_high

df_high$quintile_class_high <- rep(NA, nrow(df_high))

# assign quintile class to each observation
for (i in 1:nrow(df_high)){
  if ((quintile_values_high[1] <= df_high$prop_score_high[i]) & 
      (df_high$prop_score_high[i] < quintile_values_high[2])) {
    df_high$quintile_class_high[i] <- 1
  } else if ((quintile_values_high[2] <= df_high$prop_score_high[i]) & 
             (df_high$prop_score_high[i] < quintile_values_high[3])) {
    df_high$quintile_class_high[i] <- 2
  } else if ((quintile_values_high[3] <= df_high$prop_score_high[i]) & 
             (df_high$prop_score_high[i] < quintile_values_high[4])) {
    df_high$quintile_class_high[i] <- 3 
  } else if ((quintile_values_high[4] <= df_high$prop_score_high[i]) & 
             (df_high$prop_score_high[i] < quintile_values_high[5])) {
    df_high$quintile_class_high[i] <- 4
  } else if ((quintile_values_high[5] <= df_high$prop_score_high[i]) & 
             (df_high$prop_score_high[i] <= quintile_values_high[6])) {
    df_high$quintile_class_high[i] <- 5
  }
}

summary_high = expand.grid(
  A = c(0, 1), 
  quintile = c(1, 2, 3, 4, 5), 
  n = NA, 
  prop = NA, 
  avg_y = NA
)

for (i in 1:nrow(summary_high)) {
  subset <- df_high[(df_high$A == summary_high$A[i]) & (df_high$quintile_class_high == summary_high$quintile[i]), ]
  summary_high$n[i] = nrow(subset)
  summary_high$prop[i] = summary_high$n[i]/nrow(df_high)
  summary_high$avg_y[i] = mean(subset$Y)
}


for (i in 1:nrow(summary_high)) {
  if (is.nan(summary_high$avg_y[i]) == TRUE) {
    summary_high$avg_y[i] <- 0 
  }
}

# this table records the mean response in each quintile; needed for stratification
summary_high

quntile_prop_high <- summary_high %>% group_by(quintile) %>% summarise(sum = sum(n)/nrow(df_high))

# this table records the proportions for each quintile; also needed for stratification
quntile_prop_high

ATE_stratification_high = quntile_prop_high$sum[1]*(summary_high$avg_y[2] - summary_high$avg_y[1]) + 
  quntile_prop_high$sum[2]*(summary_high$avg_y[4] - summary_high$avg_y[3]) + 
  quntile_prop_high$sum[3]*(summary_high$avg_y[6] - summary_high$avg_y[5]) + 
  quntile_prop_high$sum[4]*(summary_high$avg_y[8] - summary_high$avg_y[7]) + 
  quntile_prop_high$sum[5]*(summary_high$avg_y[10] - summary_high$avg_y[9])


ATE_stratification_high

end.time_stratification_high <- Sys.time()
time_stratification_high <- end.time_stratification_high - start.time_stratification_high
time_stratification_high
```


#### Low Dimensional Data

The ATE of low dimension is 2.662275.  
Runtime of low dimension is 0.4974492 secs.


```{r}
start.time_stratification_low <- Sys.time()

df_low <- cbind(df_low, prop_score_low)
quintile_values_low <- rep(NA, length(quintiles))

for (i in 1:length(quintiles)){
  quintile_values_low[i] <- quantile(prop_score_low, quintiles[i])
}

# values of quintiles for low data
quintile_values_low

df_low$quintile_class_low <- rep(NA, nrow(df_low))

# assign quintile class to each observation
for (i in 1:nrow(df_low)){
  if ((quintile_values_low[1] <= df_low$prop_score_low[i]) & 
      (df_low$prop_score_low[i] < quintile_values_low[2])) {
    df_low$quintile_class_low[i] <- 1
  } else if ((quintile_values_low[2] <= df_low$prop_score_low[i]) & 
             (df_low$prop_score_low[i] < quintile_values_low[3])) {
    df_low$quintile_class_low[i] <- 2
  } else if ((quintile_values_low[3] <= df_low$prop_score_low[i]) & 
             (df_low$prop_score_low[i] < quintile_values_low[4])) {
    df_low$quintile_class_low[i] <- 3 
  } else if ((quintile_values_low[4] <= df_low$prop_score_low[i]) & 
             (df_low$prop_score_low[i] < quintile_values_low[5])) {
    df_low$quintile_class_low[i] <- 4
  } else if ((quintile_values_low[5] <= df_low$prop_score_low[i]) & 
             (df_low$prop_score_low[i] <= quintile_values_low[6])) {
    df_low$quintile_class_low[i] <- 5
  }
}


summary_low = expand.grid(
  A = c(0, 1), 
  quintile = c(1, 2, 3, 4, 5), 
  n = NA, 
  prop = NA, 
  avg_y = NA
)

for (i in 1:nrow(summary_low)) {
  subset <- df_low[(df_low$A == summary_low$A[i]) & 
                     (df_low$quintile_class_low == summary_low$quintile[i]), ]
  summary_low$n[i] = nrow(subset)
  summary_low$prop[i] = summary_low$n[i]/nrow(df_low)
  summary_low$avg_y[i] = mean(subset$Y)
}


for (i in 1:nrow(summary_low)) {
  if (is.nan(summary_low$avg_y[i]) == TRUE) {
    summary_low$avg_y[i] <- 0 
  }
}

# this table records the mean response in each quintile; needed for stratification
summary_low

quntile_prop_low <- summary_low %>% group_by(quintile) %>% summarise(sum = sum(n)/nrow(df_low))

# this table records the proportions for each quintile; also needed for stratification
quntile_prop_low

ATE_stratification_low = quntile_prop_low$sum[1]*(summary_low$avg_y[2] - summary_low$avg_y[1]) + 
  quntile_prop_low$sum[2]*(summary_low$avg_y[4] - summary_low$avg_y[3]) + 
  quntile_prop_low$sum[3]*(summary_low$avg_y[6] - summary_low$avg_y[5]) + 
  quntile_prop_low$sum[4]*(summary_low$avg_y[8] - summary_low$avg_y[7]) + 
  quntile_prop_low$sum[5]*(summary_low$avg_y[10] - summary_low$avg_y[9])

ATE_stratification_low

end.time_stratification_low <- Sys.time()
time_stratification_low <- end.time_stratification_low - start.time_stratification_low
time_stratification_low
```


### Regression Adjustment

The ATE of propensity scores under this method is the coefficients of linear regression of the whole propensity scores. The coefficients of treatment indicator show how much the treatment will affects the reponse variable.

#### High Dimensional Data 

The ATE of high dimension is -2.527116.  
Runtime of high dimension is 0.06682205 secs.


```{r}
start.time_regression_adjustment_high <- Sys.time()

ps_RA_high <- predict(tree_high, df_high, type = "prob")
high_data_ps <- cbind(ps_RA_high, df_high)
pred_high <- lm(Y ~ A + ps_RA_high, data = high_data_ps)
summary(pred_high)
ATE_regression_adjustment_high = pred_high$coefficients[2]
ATE_regression_adjustment_high

end.time_regression_adjustment_high <- Sys.time()
time_regression_adjustment_high <- end.time_regression_adjustment_high - start.time_regression_adjustment_high
time_regression_adjustment_high
```

#### Low Dimensional Data

The ATE of low dimension is 3.05324 .  
Runtime of low dimension is 0.06382895 secs.


```{r}
start.time_regression_adjustment_low <- Sys.time()

ps_RA_low <- predict(tree_low, df_low, type = "prob")
low_data_ps <- cbind(ps_RA_low, df_low)
pred_low <- lm(Y ~ A + ps_RA_low, data = low_data_ps)
summary(pred_low)
ATE_regression_adjustment_low = pred_low$coefficients[2]
ATE_regression_adjustment_low

end.time_regression_adjustment_low <- Sys.time()
time_regression_adjustment_low <- end.time_regression_adjustment_low - start.time_regression_adjustment_low
time_regression_adjustment_low
```

### Stratification and Regression Adjustment

In this section, we will combine two algorithms together. Fist, we will divide the data into 5 stratas by stratification, then we will do regression adjustment for each strata. Finally, calculate the weighted average of all stratas.

#### High Dimensional Data 

The ATE of high dimension is -2.504545 .  
Runtime of high dimension is 0.158493 secs.


```{r}
start.time_stratification_regression_adjustment_high <- Sys.time()

lm_beta_high <- rep(NA, K)

for (i in 1:K){
  subset <- df_high[df_high$quintile_class_high == i, ]
  
  if (nrow(subset) == 0) { 
    # if the quintile is empty, let the coefficient for A automatically be 0
    lm_beta_high[i] <- 0
  } else if (sum(subset$prop_score_high) == 0) { 
    # if the propensity scores in the quintile are all 0, let the coefficient for A automatically be 0
    lm_beta_low[i] <- 0
  } else {
    # otherwise, run a linear model on the subset
    lm <- lm(Y ~ A + prop_score_high, data = subset)
    lm_beta_high[i] <- as.numeric(lm$coefficients[2])
  }
}

lm_beta_high 


ATE_stratification_regression_adjustment_high <- quntile_prop_high$sum[1]*lm_beta_high[1] + 
  quntile_prop_high$sum[2]*lm_beta_high[2] + 
  quntile_prop_high$sum[3]*lm_beta_high[3] + 
  quntile_prop_high$sum[4]*lm_beta_high[4] + 
  quntile_prop_high$sum[5]*lm_beta_high[5]


ATE_stratification_regression_adjustment_high

end.time_stratification_regression_adjustment_high <- Sys.time()
time_stratification_regression_adjustment_high <- end.time_stratification_regression_adjustment_high - start.time_stratification_regression_adjustment_high
time_stratification_regression_adjustment_high
```

#### Low Dimensional Data

The ATE of low dimension is 3.058512.  
Runtime of low dimension is 0.1477561 secs.


```{r}
start.time_stratification_regression_adjustment_low <- Sys.time()

lm_beta_low <- rep(NA, 5)

for (i in 1:K){
  subset <- df_low[df_low$quintile_class_low == i, ]
  
  if (nrow(subset) == 0) {
    # if the quintile is empty, let the coefficient for A automatically be 0
    lm_beta_low[i] <- 0
  } else if (sum(subset$prop_score_low) == 0) {
    # if the propensity scores in the quintile are all 0, let the coefficient for A automatically be 0
    lm_beta_low[i] <- 0
  } else {
    # otherwise, run a linear model on the subset
    lm <- lm(Y ~ A + prop_score_low, data = subset)
    lm_beta_low[i] <- as.numeric(lm$coefficients[2])
  }
}

lm_beta_low 

ATE_stratification_regression_adjustment_low <- quntile_prop_low$sum[1]*lm_beta_low[1] + 
  quntile_prop_low$sum[2]*lm_beta_low[2] + 
  quntile_prop_low$sum[3]*lm_beta_low[3] + 
  quntile_prop_low$sum[4]*lm_beta_low[4] + 
  quntile_prop_low$sum[5]*lm_beta_low[5]

ATE_stratification_regression_adjustment_low

end.time_stratification_regression_adjustment_low <- Sys.time()
time_stratification_regression_adjustment_low <- end.time_stratification_regression_adjustment_low - start.time_stratification_regression_adjustment_low
time_stratification_regression_adjustment_low
```

## Results 

*Insert Comparison of ATE and all Runtimes Here*
*8 Runtime values*
*6 ATE estimations + 2 true ATE*
*create table and analyze results*

#Runtime results
```{r}
time <- matrix(c(time_propensity_score_high, time_stratification_high, 
                 time_regression_adjustment_high, time_stratification_regression_adjustment_high, 
                 time_propensity_score_low, time_stratification_low, time_regression_adjustment_low, 
                 time_stratification_regression_adjustment_low), ncol = 2, byrow = F)
colnames(time) <- c("High Dimensional Data", "Low Dimensional Data")
rownames(time) <- c("Propensity Score Estimation", "Stratification", "Regression Adjustment",
                    "Stratification + Regression Adjustment")
time <- as.table(time)
time
```

#ATE Results
```{r}
ATE_true_high <- -3
ATE_true_low <- 2.5
ATE <- matrix(c(ATE_true_high, ATE_stratification_high, ATE_regression_adjustment_high, 
                ATE_stratification_regression_adjustment_high, ATE_true_low, ATE_stratification_low, 
                ATE_regression_adjustment_low, ATE_stratification_regression_adjustment_low), ncol = 2, byrow = F)
colnames(ATE) <- c("High Dimensional Data", "Low Dimensional Data")
rownames(ATE) <- c("True", "Stratification", "Regression Adjustment",
                   "Stratification + Regression Adjustment")
ATE <- as.table(ATE)
ATE
```

## Conclusion 
Runtimes of propensity Score Estimation for high dimensional data is 0.704 seconds, while for low dimensional data, it's 0.095 seconds.
Among three methods, Stratification + Regression has the shortest runtimes: 0.284 seconds for high dimensinal data, and 0.335 seconds for low dimensional data.

By comparing ATE from three methods with the true ATE, we concluded that regression adjustment is the best method which estimates ATE for high dimensional data and low dimensional data that are the closest to the true ATEs.


## References 

*Description*
